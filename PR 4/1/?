Word2Vec:

Это умный способ превращать слова в векторы так, чтобы у похожих слов были близкие векторы.

Идея: если слова часто встречаются рядом в текстах (например, "кошка" и "мышь"), их векторы должны быть похожи.

Как работает:

Берутся три слова подряд (например, "кошка", "ловит", "мышь").

Компьютер учится предсказывать среднее слово ("ловит") по соседним ("кошка" и "мышь") — это CBOW.

Или наоборот: предсказывает соседние слова по одному — SkipGram.

После обучения каждое слово получает вектор (например, из 100 чисел), который сохраняет его смысл.

Примеры с векторами:

Можно находить похожие слова:
"король" - "мужчина" + "женщина" ≈ "королева".

Сравнивать слова по сходству:
similarity("мальчик", "девочка") = 0.76 (очень похожи),
similarity("машина", "коммунизм") = -0.23 (совсем не похожи).

Искать лишнее слово в списке:
doesnt_match("огонь, вода, земля, море, воздух, машина") → "машина".

Библиотека Gensim:

В коде используется готовая модель Word2Vec, обученная на большом количестве текстов.

Для английского есть мощные модели (например, на 3 млн слов), для русского — слабее (200 тыс. слов).

Этот код показывает, как слова можно превращать в числа так, чтобы компьютер "понимал" их смысл. 
Например, он может догадаться, что "король — мужчина + женщина = королева", или что "машина" не вписывается в список стихий. 
Однако такие аналогии работают лучше для английского и сильно зависят от данных, на которых училась модель.
