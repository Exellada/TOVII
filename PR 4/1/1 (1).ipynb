{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuM9XvCC23uF"
      },
      "source": [
        "# Практическая работа 4 по ТОВИИ\n",
        "# Векторные представления слов\n",
        "Выполнил Лялин Илья\n",
        "ББМО-02-24"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1FhgOr04D32"
      },
      "source": [
        "# О посимвольном представлении текста\n",
        "Несомненно, текст записывается как последовательность символов. Но человек обычно не обрабатывает каждый символ отдельно, а воспринимает \"слова\" целиком.\n",
        "\n",
        "Сможете ли вы прочитать этот [текст](https://habr.com/ru/articles/148896/) ?\n",
        "\n",
        " **По рзелульаттам илссеовадний одонго анлигйсокго унвиертисета, не иеемт занчнеия, в кокам пряокде рсапожолены бкувы в солве. Галвоне, чотбы преавя и пслоендяя бквуы блыи на мсете. Осатьлыне бкувы мгоут селдовтаь в плоонм бсепордяке, все-рвано ткест чтаитсея без побрелм. Пичрионй эгото ялвятеся то, что мы чиатем не кдаужю бкуву по отдльенотси, а все солво цликеом.**\n",
        "\n",
        "В английском варианте это звучало так:\n",
        "\n",
        "**Arocdnicg to rsceearch at Cmabrigde Uinervtisy, it deosn’t mttaer in waht oredr the ltteers in a wrod are, the olny iprmoatnt tihng is taht the frist and lsat ltteer are in the rghit pcale. The rset can be a toatl mses and you can sitll raed it wouthit pobelrm. Tihs is buseace the huamn mnid deos not raed ervey lteter by istlef, but the wrod as a wlohe.**\n",
        "\n",
        "Таким образом, текст выгодней представлять не как последовательность символов, а как последовательность слов.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psYi8z3h23uT"
      },
      "source": [
        "# Унитарное кодирование\n",
        "Пожалуй самым простым векторным представлением является *унитарное кодирование* (по-английски one-hot encoding).\n",
        "\n",
        "Возьмем какой-либо текст, разобьем его на слова-элементы и составим словарь таких слов, в котором каждому слову будет приписан его номер в словаре.\n",
        "\n",
        "Этот номер слова можно представить в бинарном виде, сопоставим каждому слову в словаре *бинарный вектор*, состоящий из нулей и только одной единицы. Положение единицы в векторе показывает номер слова.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1hDY2EKB7ND-Uxq9FnKVtxC-U-0bnuiTe)\n",
        "\n",
        "В векторе будет столько элементов, сколько слов в словаре мы записали.\n",
        "\n",
        "Обратите внимание, что единичка, по сути, является признаком слова: стоит единичка в пятом элементе, значит слово номер пять.\n",
        "\n",
        "Вообще, в словарь мы можем добавлять какие угодно \"слова\", это могут быть обычные привычные нам слова, части слов и даже отдельные буквы, словосочетания и целые предложения, знаки препинания, смайлики, и прочее. Главное, что у всех этих \"слов\" есть номер, представленный унитарным вектором.\n",
        "\n",
        "Текст является последовательностью слов, а в унитарном представлении - последовательностью унитарных векторов.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1uEodrFkeG433O_zCWf3EpR8JVPYEFysx)\n",
        "\n",
        "Унитарное кодирование используется и для других типов информации, для изображений, сигналов и пр. Хорошим свойством является то, что унитарное представление не зависит от длины слова, ведь в словарь мы можем добавлять слова разной длины, хоть из одной буквы, хоть целое словосочетание.\n",
        "\n",
        "Однако такое представление не дает нам никаких новых интересных свойств, ведь унитарные вектора никак не связаны между собой и зависят от нашего произвола - в каком порядке слова в словарь добавляем, такой вектор и получим.\n",
        "\n",
        "Поэтому создают другие, более интересные, векторные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_7KaRdJ23uT"
      },
      "source": [
        "# Word2vec\n",
        "Хочется как-то связать между собой слова и их векторные представления, ведь в реальных текстах слова, конечно же, связаны между собой **смыслом**. Вряд ли кто-то захочет читать тексты из случайных слов.\n",
        "\n",
        "Если есть взаимосвязь между словами текста, то ее можно уловить, вычислить.\n",
        "\n",
        "А давайте попробуем сделать так:\n",
        "- возьмем из текста три слова подряд.\n",
        "- закроем одно слово и попробуем его угадать по двум оставшимся.\n",
        "\n",
        "Во многих случаях, но конечно не всегда, это нам удастся. Попробуйте угадать какое слово скрыто под звездочками во фразе \"Мама мыла ....\".\n",
        "Если вы сказали \"раму\", то угадали, но могли сказать \"пол\" и не угадали.\n",
        "\n",
        "Давайте возьмем больше слов \"Мама мыла оконную ...\": тут легче угадать слово \"раму\", но все равно есть и другие варианты (придумайте).\n",
        "\n",
        "И все же, для многих последовательностей слов можно угадать какое слово закрыто, так пусть этим займется компьютер.\n",
        "\n",
        "Текст это последовательность слов, слова можно представить унитарными векторами. Нам надо лишь по нескольким унитарным векторам вычислять следующий. Это абсолютно то же самое, как если бы мы хотели обучить нейронную сеть. Известны входы - несколько унитарных векторов, знаем выход - один унитарный вектор, давайте обучим.\n",
        "\n",
        "Можно посмотреть и с другой стороны, если по нескольким словам мы угадываем следующее, то давайте наоборот, по слову угадаем какие слова ему предшествовали. Технически задача такая же. Знаем вход - один унитарный вектор, знаем выход - несколько унитарных векторов - так давайте обучим нейронную сеть.\n",
        "\n",
        "Эти подходы можно совместить, по нескольким векторам угадываем один пропущенный, а затем по нему угадываем обратно какие вектора были изначально, или, другими словами, угадываем слово по словам его окружающим, а затем по этому слову угадываем окружающие слова. Обычно угадывают среднее слово из трех, но это вовсе не обязательно.\n",
        "\n",
        "Такая технология получила название **word2vec** (произносится \"ворд ту век\"). На картинке показана для пяти слов, обозначенных буквой V с индексом в скобках.  Первую половинку - угадывание слова по окружению - назвали CBOW, вторую - угадывания окружения по слову - SkipGram.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=18pa20uv6xi8tGC6pdOS3Vc11KUUcpW2e)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CBOW предсказывает текущее слово на основе контекста вокруг него. Например, для фразы «синее небо над головой» модель CBOW будет пытаться предсказать слово «небо» на основе контекстных слов «синее», «над», «головой». CBOW быстро обрабатывает большие объёмы данных, но менее эффективен для редких слов.\n",
        "\n",
        "Skip-Gram, в отличие от CBOW, предсказывает контекстные слова для данного целевого слова. Для того же примера модель Skip-Gram будет пытаться предсказать слова «синее», «над», «головой» на основе слова «небо». Skip-Gram медленнее обрабатывает данные, но лучше работает с редкими словами и менее частыми контекстами."
      ],
      "metadata": {
        "id": "58ep7Xw56fUi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeHc1Oxn23uU"
      },
      "source": [
        "Посмотрим на CBOW (это сокращение от Continuous Bag of Word, непрерывный мешок слов).\n",
        "\n",
        "Мы можем использовать простую нейронную сеть только с одним скрытым слоем для него.\n",
        "\n",
        "Унитарные вектора слов-окружения переводятся в скрытом слое в некоторые выходы, а затем по выходам этого скрытого слоя считается унитарный вектор исходного слова.\n",
        "\n",
        "Перевод унитарного представления слов-окружения в скрытом слое производится, как мы понимаем из работы нейронной сети, умножением на матрицу весов W. Ее размер (число нейронов в скрытом слое) * (число слов в словаре). Нет необходимости делать эту матрицу разной для разных слов-окружений, пусть будет одна.\n",
        "\n",
        "Аналогично, переход из скрытого слоя на выход также делается с помощью умножения на матрицу W'. Ее размер (число слов в словаре) * (число нейронов в скрытом слое).\n",
        "\n",
        "Матрица W' нам не важна, но посмотрим более внимательно на матрицу W.\n",
        "\n",
        "Она умножается на унитарный вектор, т.е. вектор, в котором все элементы нули кроме одного. Что произойдет при таком умножении?\n",
        "\n",
        "Правильно, выберется только один столбец из матрицы, с таким номером, на какой позиции стоит единичка во входом векторе. Но положение этой единички означает номер слова в словаре! Значит, каждому слову из словаря соответствует свой вектор-столбец в матрице W. Его размерность определяется числом нейронов в скрытом слое, которое мы задаем сами при обучении. **Эти вектора-столбцы и являются новым векторным представлением слов**.   \n",
        "\n",
        "Итак, вкратце, еще раз.\n",
        "- Берем тексты\n",
        "- разбиваем их на последовательность слов\n",
        "- переводим слова в унитарные вектора\n",
        "- на парах (соседние слова)-(среднее слово) обучаем простую нейронную сеть CBOW с ее матрицей W.\n",
        "- используем столбцы матрицы W как новое векторное представление слов. Длина этих векторов задается произвольно при обучении.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1bTpfqH0niwJKwMRHKdNbuOFiR9jucCZe)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hqo4y3v523uV"
      },
      "source": [
        "# Геометрия слов\n",
        "Создав векторные представления для слов, можно их использовать в различных *геометрических* операциях: сложить вектора, вычесть, найти угол меду ними.\n",
        "\n",
        "И, что наиболее удивительно в таких векторных представлениях, эти геометрические операции с векторами имеют **смысл** с точки зрения самих слов.\n",
        "\n",
        "Например:\n",
        "* Король - мужчина + женщина = Королева\n",
        "* Великобритания - Лондон + Москва = Россия\n",
        " и др.\n",
        "\n",
        " Прежде чем смотреть на примеры, несколько слов о сравнении векторов.\n",
        "\n",
        "## Косинусное расстояние.\n",
        "\n",
        "Вектора можно сравнивать между собой, логично, что близкие вектора означают близкие по смыслу слова. Но как именно сравнивать вектора?\n",
        "\n",
        "Оказалось, что простое Евклидово расстояние между векторами не так интересно, более интересно сравнивать угол между векторами или, правильнее, косинус угла.\n",
        "\n",
        "Ну-ка, вспоминайте геометрию, как посчитать косинус угла между векторами?\n",
        "\n",
        "А вот так: скалярное произведение векторов поделить на длины этих векторов.\n",
        "\n",
        "![img](https://drive.google.com/uc?id=1vxVWdSAjY5oK6U7CtGadB0-2T5YLFmSh)\n",
        "\n",
        "Часто векторные представления *нормализуют*, т.е. приводят к единичной длине, тогда ее и считать не надо.\n",
        "\n",
        "Итак, мера схожести векторов - косинус угла между ними. Близкие по смыслу слова скорей всего дадут близкие по углу вектора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BOipolf23uW"
      },
      "source": [
        "# Библиотека gensim\n",
        "\n",
        "А теперь примеры. Поиграть с векторами онлайн вы можете [здесь](https://rare-technologies.com/word2vec-tutorial/#app), но давайте и сами реализуем.  \n",
        "\n",
        "Нам поможет библиотека [gensim](https://radimrehurek.com/gensim/index.html). Установим ее.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IXyn2JWv23uW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a942418-f83e-4c55-b503-bd64c0068b05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m737.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim # установка gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0J6XlEf23uY"
      },
      "source": [
        "Подключим вспомогательную библиотеку для записи действий."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PtqCCWh423uY"
      },
      "outputs": [],
      "source": [
        "import logging # Импорт модуля logging, который предоставляет функционал для логирования (записи событий) в приложениях\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) #  устанавливает минимальный уровень логируемых сообщений (INFO и выше)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2HHRtF923uZ"
      },
      "source": [
        "Скачаем уже обученную модель [word2vec](https://radimrehurek.com/gensim/models/word2vec.html), она была обучена на огромном наборе текстов на английском языке из 3 млн. слов и занимает около 2 Гигабайт, придется подождать. Это надо сделать только один раз. Модель загружается в виде специального объекта, в котором прописаны многие методы для работы с векторами.\n",
        "\n",
        "*Для учителя: иногда сайт обрывает связь и закачку, рекомендуется заранее скачать массивы используя методы load(), save(), или используйте меньшие вектора, или скачайте заранее и поместите в директорию /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zw26YKfY23ua"
      },
      "outputs": [],
      "source": [
        "#import gensim.downloader as api\n",
        "#wv = api.load('word2vec-google-news-300') # большая модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptoeU-ro-YHI"
      },
      "source": [
        "Воспользуемся корпусом поменьше и обучим на нем модель word2vec."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYlsq8FeRkAR",
        "outputId": "e50fa78e-4450-4961-8a4b-799cb21ea4df"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.5)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f_q5qYCi3txa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c530ffb-b965-4703-c46d-204da828e4e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Затем перезапустим среду выполнения (это важно!)\n",
        "# Нажмите Ctrl+M . или выберите в меню Runtime -> Restart runtime\n",
        "\n",
        "# Теперь импортируем библиотеки\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "# Загружаем корпус и обучаем модель\n",
        "corpus = api.load('text8')\n",
        "model = Word2Vec(corpus)\n",
        "wv = model.wv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Корпус text8 — это предобработанный текстовый набор данных, который состоит из одного длинного файла с токенизированными словами (в нижнем регистре, без пунктуации и цифр).\n",
        "\n",
        "Формат: последовательность слов (токенов) без разделения на предложения или абзацы.\n",
        "\n",
        "Пример содержимого: \"cat set on ...\" (сплошной поток слов).\n",
        "\n",
        "Предобработка: удалены все не буквенные символы, слова приведены к lowercase.\n",
        "\n",
        "Модель Word2Vec сама разбивает этот поток на \"предложения\" (по умолчанию окном в 5 слов) при обучении."
      ],
      "metadata": {
        "id": "Zlu7pBV3lVR2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prcvk_j023ua"
      },
      "source": [
        "Выведем на экран первые 10 слов из словаря, которые хранятся в поле .index_to_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeBsRaP_23ub",
        "outputId": "c5595bfa-c594-40da-8292-ddeea780e232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word #0/71290 is the\n",
            "word #1/71290 is of\n",
            "word #2/71290 is and\n",
            "word #3/71290 is one\n",
            "word #4/71290 is in\n",
            "word #5/71290 is a\n",
            "word #6/71290 is to\n",
            "word #7/71290 is zero\n",
            "word #8/71290 is nine\n",
            "word #9/71290 is two\n"
          ]
        }
      ],
      "source": [
        "for index, word in enumerate(wv.index_to_key): # Цикл по всем словам из словаря модели с их индексами\n",
        "# wv.index_to_key - список слов в словаре модели, упорядоченных по частоте\n",
        "# enumerate() возвращает пары (индекс, слово) для каждого элемента списка\n",
        "\n",
        "    if index == 10:  # если индекс текущего слова равен 10\n",
        "        break\n",
        "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\") # вывод"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqjsNfe223ub"
      },
      "source": [
        "Получить вектор слова можно используя это слово как индекс. Получим для слова 'king'. Это вектор из 100 элементов. Конечно, это слово должно быть в словаре и именно в таком виде, иначе возникнет ошибка. Важен и регистр букв, попробуйте слово 'kinG'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7rmj06m23uc",
        "outputId": "add02335-e6e2-4e69-96d0-e6c2026ded32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.59204865  0.62552214  1.6119288   0.988316    1.2312654   0.10949396\n",
            "  2.1560488   1.4056922  -0.04452587  1.7421284  -2.2102048  -0.78181684\n",
            "  1.5348539   2.856291    0.15364572  0.25816777  2.2194479   0.8294913\n",
            "  4.8914137   1.6528162   1.2144706   2.518574   -1.7041585   2.0466561\n",
            "  0.15431817  2.7615094   0.7935868   0.38603988  0.5659158   1.3087512\n",
            " -2.1744807  -0.19884476 -1.9574934   1.5222707   0.8229407   0.2016539\n",
            "  0.90043676 -0.4976294  -1.7559888  -0.15877396 -1.656721    1.1686856\n",
            "  1.1321297  -0.60299    -1.5758268  -2.3299098  -1.3292221   0.10407055\n",
            "  1.8937558  -1.717183    1.7304164   1.0487123  -1.7959898   2.4789903\n",
            " -0.27814192 -3.1642811   0.0957384  -0.44148678  2.785002   -0.684294\n",
            " -0.7668499  -0.45599195  1.8719044  -1.0169792   0.1510676  -2.1215506\n",
            " -0.57992315  0.89829314  0.8201503  -2.1561577   1.6590822  -0.4585004\n",
            " -2.290902   -1.6867777   1.3324794  -2.80487     1.5070877   0.06373303\n",
            " -3.748682    2.2862716  -1.4430743  -4.7372127  -2.9296021   0.47236133\n",
            " -0.885404    1.9388529   2.1608233   0.3422355  -1.6992363  -0.24855486\n",
            "  0.84263664 -1.5569961  -0.44303185 -2.2471178   1.9359006  -1.2939811\n",
            "  1.7983193  -1.0190258  -2.3571036  -1.7881508 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "vec_king = wv['king'] # Получаем векторное представление (эмбеддинг) слова \"king\" из модели Word2Vec\n",
        "print(vec_king)\n",
        "vec_king.shape # Выводим форму (размерность) вектора слова \"king\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V3CduHpM_u80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "8242a7e2-b0ca-4058-9db7-3076f8534cdd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Key 'King' not present\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-ea888bab6efc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_king\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'King'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# такого слова нет в словаре\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key_or_keys)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey_or_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkey_or_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m         \"\"\"\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Key 'King' not present\""
          ]
        }
      ],
      "source": [
        "vec_king = wv['King'] # такого слова нет в словаре"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uzk2QWmI23uc"
      },
      "source": [
        "Посчитать \"похожесть\" слов, т.е. их векторов, можно с помощью метода `.similarity()`. Чем больше число, тем более похожи слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvdK5kQR23uc",
        "outputId": "af472019-c3ed-43bf-d421-2c6ff6024ba6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'car'\t'minivan'\t0.32\n",
            "'car'\t'bicycle'\t0.63\n",
            "'car'\t'airplane'\t0.53\n",
            "'car'\t'cereal'\t0.15\n",
            "'car'\t'communism'\t-0.23\n"
          ]
        }
      ],
      "source": [
        "pairs = [                 # Создаем список кортежей (пар слов) для проверки их семантической близости\n",
        "# Каждая пара содержит два слова, между которыми мы хотим измерить сходство\n",
        "    ('car', 'minivan'),   # высокая схожесть - оба слова обозначают транспортные средства\n",
        "    ('car', 'bicycle'),   # средняя схожесть - оба являются видами транспорта, но разного типа\n",
        "    ('car', 'airplane'),  # некоторая схожесть - оба средства передвижения, но разной категории\n",
        "    ('car', 'cereal'),    # низкая схожесть - практически нет связи между понятиями\n",
        "    ('car', 'communism'), # очень низкая/нулевая схожесть - совершенно разные понятия\n",
        "]\n",
        "# Начинаем итерацию по всем парам слов из списка pairs\n",
        "# На каждой итерации w1 и w2 получают значения из текущей пары\n",
        "for w1, w2 in pairs:\n",
        "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.similarity('boy', 'girl'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQKW_iTo_7l8",
        "outputId": "151d9a44-e815-425f-b127-c0e6ad386141"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7644291\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.similarity('boy', 'man'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65wsIZktAO8N",
        "outputId": "0104ee24-bc5e-463e-e2ab-271e119ffd23"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.519774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.similarity('man', 'woman'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBEpKfUtASyj",
        "outputId": "b3a8f5aa-efcd-4a96-fd8b-7d507dc3eeda"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7392697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Забавно)\n",
        "\n",
        "Думаю, так получается, потому что модель сделала акцент больше на \"возрасте\" слов, чем на \"однотипности\" слов. Собственно, значит в обучающем наборе данных, в большинстве случаев, логичнее было делать замену 'boy' <-> 'girl' (с учетом контекста)."
      ],
      "metadata": {
        "id": "xMbJ6knfAX8M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huAtz1iv23ud"
      },
      "source": [
        "Перебирать вручную все пары слов, чтобы найти самые похожие будет трудно, пусть компьютер сравнит все-все вектора и выведет самые похожие на заданный.\n",
        "\n",
        "Метод `.most_similar()` принимает набор слов для которых искать похожие (аргумент positive) и число похожих слов (аргумент topn)\n",
        "\n",
        "(Первый запуск может быть долгим, так как надо посчитать длины всех векторов, потом будет гораздо быстрее)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "29vJFCWf23ud",
        "outputId": "1ed0e00a-8a98-4ff5-b230-ce4514dd31fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('girl', 0.7737048864364624), ('child', 0.6981321573257446), ('stranger', 0.6798149943351746), ('person', 0.6682233810424805), ('lady', 0.6639747619628906)]\n",
            "[('hamster', 0.6995736360549927), ('breed', 0.6764039993286133), ('bird', 0.6577634215354919), ('cow', 0.6499366760253906), ('greyhound', 0.6280971169471741)]\n"
          ]
        }
      ],
      "source": [
        "print(wv.most_similar(positive=['man', 'woman'], topn=5)) # поиск 5 наиболее схожив слов\n",
        "print(wv.most_similar(positive=['cat','dog'],negative=['duck'], topn=5)) # так же поиск наиболее схожих слов, но без утки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av4fcfol23ue"
      },
      "source": [
        "Найти слово, которое непохоже на остальные? Легко, метод .doesnt_match(). Все слова (их вектора) будут сравнены между собой и выведется то, которое похоже меньше всех."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYnqSCA823ue",
        "outputId": "bd533a12-d8ab-42ba-d6a5-038c7524b5da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "car\n"
          ]
        }
      ],
      "source": [
        "print(wv.doesnt_match(('fire', 'water', 'land', 'sea', 'air', 'car'))) # определение лишнего слова в наборе"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.doesnt_match(('honest', 'kindness', 'wisdom', 'seen')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cU7R8QOrSfu6",
        "outputId": "3c17813f-5b6f-4ccb-cfaf-7077b9ff9f9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wv.doesnt_match(('earth', 'sun', 'space')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT2_rTKKSSx4",
        "outputId": "9ce1c0e0-191e-467e-9297-7602e0c2fe99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "space\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwgLBaTM23ue"
      },
      "source": [
        "Геометрические операции.\n",
        "\n",
        "Вектор 'France' минус вектор 'Paris' плюс вектор 'Moscow'. Получится какой-то вектор. Его может не быть в словаре, найдем ближайшие из словаря к нему, метод `.similar_by_vector()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCYoxEHB23ue",
        "outputId": "0af84cae-8a72-44ed-9465-c3caa98c54cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('russia', 0.7316857576370239),\n",
              " ('finland', 0.7273222804069519),\n",
              " ('lithuania', 0.7049739956855774),\n",
              " ('moldova', 0.6978358030319214),\n",
              " ('libya', 0.6865296959877014),\n",
              " ('eritrea', 0.6864036917686462),\n",
              " ('mozambique', 0.6863219738006592),\n",
              " ('afghanistan', 0.6803471446037292),\n",
              " ('france', 0.6706655025482178),\n",
              " ('cyprus', 0.6667357087135315)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "Russia=wv['france']-wv['paris']+wv['moscow'] #  создание вектора Россия где из Франции вычитатся Париж и прибавляется Москва\n",
        "wv.similar_by_vector(Russia) # Ищем 10 наиболее близких слов к полученному вектору России"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GifuP9hR1-_",
        "outputId": "cf0ee99c-3d90-4ed0-8dc4-9da5ce8859f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('confusing', 0.5617622137069702),\n",
              " ('transitive', 0.5609015226364136),\n",
              " ('slippery', 0.5602009892463684),\n",
              " ('reactive', 0.5521301031112671),\n",
              " ('haggis', 0.5313898921012878),\n",
              " ('desirable', 0.5234452486038208),\n",
              " ('soft', 0.5223643183708191),\n",
              " ('inherently', 0.5214518308639526),\n",
              " ('hamsters', 0.5197364091873169),\n",
              " ('atypical', 0.5174958109855652)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "q=wv['drunk']-wv['was']+wv['is']\n",
        "wv.similar_by_vector(q)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q=wv['education']-wv['teacher']+wv['tutorial']\n",
        "wv.similar_by_vector(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpn6qdFIT0zp",
        "outputId": "ef329bd7-d8fa-4bc1-c5b9-10a3c70cdde5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('education', 0.7082630395889282),\n",
              " ('healthcare', 0.6780317425727844),\n",
              " ('banking', 0.6522578001022339),\n",
              " ('infrastructure', 0.6468461751937866),\n",
              " ('accounting', 0.6468152403831482),\n",
              " ('resource', 0.6382452249526978),\n",
              " ('transportation', 0.6168580055236816),\n",
              " ('funding', 0.615738570690155),\n",
              " ('telecommunications', 0.6113738417625427),\n",
              " ('educational', 0.6092763543128967)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обычно слова, учавствующие в исходном примере, не учитываются в результатх!"
      ],
      "metadata": {
        "id": "hTPynMISVFjZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QS6pGzd23ue"
      },
      "source": [
        "А на русском языке??\n",
        "\n",
        "Такие модели тоже есть, но они гораздо слабее, мало слов, мало текстов.\n",
        "Загрузим модель word2vec-ruscorpora-300 . Всего-то 200 тысяч слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pOWgBWZa23ug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9294a07d-d2d9-4560-8566-7daf67b0c6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 198.8/198.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "wv_rus = api.load('word2vec-ruscorpora-300') # загрузка модели русской речи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTcErjx923ug"
      },
      "source": [
        "Здесь слова имеют приставки, показывающие их часть речи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "175L8SiI23ug"
      },
      "source": [
        "Что получится для король - мужчина + женщина ?\n",
        "По идее королева. Но получится король. Но и \"правильный\" ответ тоже недалеко, второй по счету.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBtwTl2n23ug",
        "outputId": "39b32e1d-c7e2-4040-d271-458dece8df8a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('король_NOUN', 0.8805387616157532),\n",
              " ('королева_NOUN', 0.7313904762268066),\n",
              " ('герцог_NOUN', 0.6502388715744019),\n",
              " ('принцесса_NOUN', 0.6266285181045532),\n",
              " ('герцогиня_NOUN', 0.6240381598472595),\n",
              " ('королевство_NOUN', 0.6094207167625427),\n",
              " ('зюдерманландский_ADJ', 0.6084389686584473),\n",
              " ('дурлахский_ADJ', 0.6081665754318237),\n",
              " ('ульрик::элеонора_NOUN', 0.6073107719421387),\n",
              " ('максимилианов_NOUN', 0.6057003736495972)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "queen= wv_rus['король_NOUN'] - wv_rus['мужчина_NOUN'] + wv_rus['женщина_NOUN']\n",
        "\n",
        "wv_rus.similar_by_vector(queen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuD6ipd223uh"
      },
      "source": [
        "# Задания\n",
        "Успешность геометрических операций с векторами слов сильно зависит и от вида модели и от текстов, на которых она обучалась. Не все понятные человеку аналогии слов, получаются и в компьютере. В качестве упражнения попробуйте найти примеры логичных и нелогичных аналогий."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_rus = wv_rus['звук_NOUN'] - wv_rus['радио_NOUN'] + wv_rus['кино_NOUN']  # загадано слово \"фильм\"\n",
        "wv_rus.similar_by_vector(vector_rus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WzUm1uFhxP1",
        "outputId": "b3910099-4bcf-4eab-a160-d9a2264eaf6b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('звук_NOUN', 0.6083037257194519),\n",
              " ('кино_NOUN', 0.51164311170578),\n",
              " ('музыка_NOUN', 0.43696707487106323),\n",
              " ('зосина_NOUN', 0.4338759779930115),\n",
              " ('саспенс_NOUN', 0.4294145405292511),\n",
              " ('кинематографичность_NOUN', 0.42716899514198303),\n",
              " ('фильм_NOUN', 0.4269719421863556),\n",
              " ('марсель::карн_NOUN', 0.42510953545570374),\n",
              " ('киношный_ADJ', 0.4228109121322632),\n",
              " ('мадо_NOUN', 0.41768062114715576)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример взаимосвязи, понятной как человеку, так и модели."
      ],
      "metadata": {
        "id": "AYPCzd5xcnb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vector_rus = wv_rus['квакать_VERB'] - wv_rus['лягушка_NOUN'] + wv_rus['шипеть_VERB']  # загадано слово \"жаба\"\n",
        "wv_rus.similar_by_vector(vector_rus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCgbCssOgn3d",
        "outputId": "4869a97c-9533-402e-d3b9-e9d4892d89eb"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('шипеть_VERB', 0.7393226623535156),\n",
              " ('квакать_VERB', 0.5753850340843201),\n",
              " ('шипение_NOUN', 0.5316094756126404),\n",
              " ('гудеть_VERB', 0.5221823453903198),\n",
              " ('рычать_VERB', 0.5180732607841492),\n",
              " ('орать_VERB', 0.5136971473693848),\n",
              " ('хрипеть_VERB', 0.5101461410522461),\n",
              " ('визжать_VERB', 0.5039240717887878),\n",
              " ('свистеть_VERB', 0.49936574697494507),\n",
              " ('бубнить_VERB', 0.49717721343040466)]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_rus = wv_rus['шипеть_VERB'] - wv_rus['лягушка_NOUN'] + wv_rus['квакать_VERB']  # загадано слово \"жаба\"\n",
        "wv_rus.similar_by_vector(vector_rus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "halFll-LTRvw",
        "outputId": "35e9c281-ec24-4121-a924-84f4f2216516"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('шипеть_VERB', 0.7393226623535156),\n",
              " ('квакать_VERB', 0.5753850340843201),\n",
              " ('шипение_NOUN', 0.5316095352172852),\n",
              " ('гудеть_VERB', 0.5221824049949646),\n",
              " ('рычать_VERB', 0.5180732607841492),\n",
              " ('орать_VERB', 0.5136971473693848),\n",
              " ('хрипеть_VERB', 0.5101461410522461),\n",
              " ('визжать_VERB', 0.5039240717887878),\n",
              " ('свистеть_VERB', 0.49936574697494507),\n",
              " ('бубнить_VERB', 0.49717721343040466)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Модель ищет в своем словаре слова, чьи векторы ближе всего к полученному vector_rus."
      ],
      "metadata": {
        "id": "qw9zNeQpejXV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Человеку такая связь понятна, но модель её не улавливает — возможно, из-за сложной логики или редких сочетаний в данных. Вместо этого модель даёт буквальные ассоциации (например, 'металлы'). И, возможно, к лучшему: если бы модель искала скрытые смыслы везде, результаты были бы менее предсказуемыми."
      ],
      "metadata": {
        "id": "1o3iuWpycyKL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "il0jKMxH23uh"
      },
      "source": [
        "# Заключение\n",
        "Сегодня существует множество моделей для векторных представлений слов на разных языках. По похожему принципу строятся и векторные представления предложений и даже текстов целиком. Мы еще познакомимся с некоторыми примерами в этой области."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHQmJaAJ23ui"
      },
      "source": [
        "## Ссылки\n",
        "\n",
        "Использованы и адаптированы материалы:\n",
        "* https://radimrehurek.com/gensim/index.html\n",
        "* https://medium.com/sciforce/word-vectors-in-natural-language-processing-global-vectors-glove-51339db89639\n",
        "* https://colab.research.google.com/github/mdda/deep-learning-workshop/blob/master/notebooks/5-RNN/3-Text-Corpus-and-Embeddings.ipynb#scrollTo=h-CQETk6HPmx\n",
        "* https://radimrehurek.com/gensim/models/fasttext.html\n",
        "* https://amitness.com/2020/06/fasttext-embeddings/\n",
        "* http://vectors.nlpl.eu/explore/embeddings\n",
        "\n",
        "Рекомендую посмотреть курсы:\n",
        "* First Step in NLP https://stepik.org/course/129443/syllabus\n",
        "* Second Step in NLP https://stepik.org/course/133963/syllabus"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}